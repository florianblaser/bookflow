{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Settings ---\n",
    "folder_path = 'data/100_clean'\n",
    "FREQUENCY_THRESHOLD = 100\n",
    "throttle_delay = 1  # seconds delay between processing files\n",
    "dict_file = 'difficult_word_to_help.json'\n",
    "freq_file = 'data/es_frequencies.csv'\n",
    "difficult_words_file = 'difficult_words.json'  # New file to store scanned difficult words\n",
    "\n",
    "# --- Load frequency data and create a lookup dictionary ---\n",
    "freq_df = pd.read_csv(freq_file)\n",
    "# Use lower-case keys for consistency.\n",
    "freq_dict = {str(word).lower(): freq for word, freq in zip(freq_df['Word'], freq_df['Freq_count'])}\n",
    "\n",
    "\n",
    "def is_difficult(word):\n",
    "    \"\"\"Return True if the word's frequency is below threshold.\"\"\"\n",
    "    freq = freq_dict.get(word)\n",
    "    return freq is not None and freq < FREQUENCY_THRESHOLD\n",
    "\n",
    "# --- Process XHTML files or load existing difficult words ---\n",
    "difficult_words = set()\n",
    "file_list = [f for f in os.listdir(folder_path) if f.endswith(('.xhtml', '.htm'))]\n",
    "start_time = time.time()\n",
    "for filename in tqdm(file_list, desc=\"Processing files\"):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        words = re.findall(r'\\b\\w+\\b', text, flags=re.UNICODE)\n",
    "        for word in words:\n",
    "            lower_word = word.lower()\n",
    "            if is_difficult(lower_word):\n",
    "                difficult_words.add(lower_word)\n",
    "    time.sleep(throttle_delay)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Processed {len(file_list)} files in {elapsed:.2f} seconds.\")\n",
    "print(f\"Found {len(difficult_words)} difficult words.\")\n",
    "\n",
    "with open(difficult_words_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(list(difficult_words), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# --- Load existing dictionary if it exists ---\n",
    "if os.path.exists(dict_file):\n",
    "    with open(dict_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    # If loaded data is a list, convert it to a dict.\n",
    "    if isinstance(data, dict):\n",
    "        difficult_word_to_help = data\n",
    "    else:\n",
    "        difficult_word_to_help = {}\n",
    "else:\n",
    "    difficult_word_to_help = {}\n",
    "\n",
    "remaining_words = difficult_words - set(difficult_word_to_help.keys())\n",
    "difficult_words_list = list(remaining_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 2 (words 1 to 100)\n",
      "Processing batch 2 of 2 (words 101 to 127)\n"
     ]
    }
   ],
   "source": [
    "if not difficult_words_list:\n",
    "    print(\"No new difficult words to process. Exiting.\")\n",
    "\n",
    "# Initialize Groq client.\n",
    "client = Groq(api_key='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')\n",
    "\n",
    "# Determine total batches (each batch of 100 words).\n",
    "batch_size = 100\n",
    "total_batches = (len(difficult_words_list) + batch_size - 1) // batch_size\n",
    "\n",
    "# Process difficult words in batches.\n",
    "for batch_index in range(total_batches):\n",
    "    start = batch_index * batch_size\n",
    "    batch = difficult_words_list[start:start + batch_size]\n",
    "    print(f\"Processing batch {batch_index + 1} of {total_batches} (words {start + 1} to {start + len(batch)})\")\n",
    "    \n",
    "    words_str = \", \".join(batch)\n",
    "    prompt = (\n",
    "        f\"Provide a defining translation in English for each of the following words: {words_str}. \"\n",
    "        \"Use no more than three words per defining translation. \"\n",
    "        \"Return the result as a JSON array where each element is an object with the keys 'word' and 'help'. \"\n",
    "        \"Do not include any extra text outside the JSON.\"\n",
    "    )\n",
    "\n",
    "    # First attempt with llama-3.3-70b-versatile.\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            stream=False,\n",
    "        )\n",
    "        response_text = chat_completion.choices[0].message.content.strip()\n",
    "        definitions = json.loads(response_text)\n",
    "        for entry in definitions:\n",
    "            word_key = entry['word'].lower()  # Normalize keys to lowercase.\n",
    "            difficult_word_to_help[word_key] = entry['help']\n",
    "    except Exception:\n",
    "        # Second attempt with gemma2-9b-it.\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"gemma2-9b-it\",\n",
    "                stream=False,\n",
    "            )\n",
    "            response_text = chat_completion.choices[0].message.content.strip()\n",
    "            definitions = json.loads(response_text)\n",
    "            for entry in definitions:\n",
    "                word_key = entry['word'].lower()\n",
    "                difficult_word_to_help[word_key] = entry['help']\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to parse JSON for batch: {batch}\")\n",
    "            print(\"Response was:\")\n",
    "            print(response_text)\n",
    "\n",
    "    # Optional throttle delay.\n",
    "    time.sleep(throttle_delay)\n",
    "\n",
    "# Save the updated dictionary locally.\n",
    "with open(dict_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(difficult_word_to_help, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total changes made across all files: 157\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Process each XHTML file and replace difficult words with \"original_word [help]\".\n",
    "total_changes = 0  # Track total changes across all files\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(('.xhtml', '.htm')):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        # Build a regex pattern matching any word from difficult_words_list (case-insensitive)\n",
    "        pattern = re.compile(\n",
    "            r'\\b(' + '|'.join(re.escape(word) for word in difficult_words_list) + r')\\b',\n",
    "            flags=re.IGNORECASE\n",
    "        )\n",
    "\n",
    "        def replacer(match):\n",
    "            global total_changes  # Use global keyword to modify the module-level variable\n",
    "            original_word = match.group(0)\n",
    "            # Lookup using the dictionary (normalized to lowercase).\n",
    "            help_text = difficult_word_to_help.get(original_word.lower(), '')\n",
    "            if help_text:\n",
    "                total_changes += 1  # Increment total changes for each replacement\n",
    "                return f\"{original_word} [{help_text}]\"\n",
    "            return original_word\n",
    "\n",
    "        new_content = pattern.sub(replacer, content)\n",
    "\n",
    "        # Overwrite the file with the updated content.\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(new_content)\n",
    "\n",
    "# Print the total number of changes\n",
    "print(f\"Total changes made across all files: {total_changes}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
