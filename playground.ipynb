{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import textstat\n",
    "from groq import Groq\n",
    "\n",
    "folder_path = 'data/100_clean'\n",
    "CEFR_LEVEL_THRESHOLD = 20\n",
    "throttle_delay = 1\n",
    "\n",
    "def is_difficult(word):\n",
    "    try:\n",
    "        return textstat.flesch_kincaid_grade(word) > CEFR_LEVEL_THRESHOLD\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Step 1: Collect all difficult words from all xHTML files.\n",
    "difficult_words = set()\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.xhtml'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "            text = soup.get_text()\n",
    "            words = re.findall(r'\\b\\w+\\b', text, flags=re.UNICODE)\n",
    "            for word in words:\n",
    "                lower_word = word.lower()\n",
    "                if is_difficult(lower_word):\n",
    "                    difficult_words.add(lower_word)\n",
    "\n",
    "# Step 2: Build a dictionary mapping each difficult word to its \"help\" definition.\n",
    "client = Groq(api_key='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')\n",
    "difficult_word_to_help = {}\n",
    "difficult_words_list = list(difficult_words)\n",
    "\n",
    "# Process difficult words in batches of 100 until 80 requests have been made.\n",
    "for i in range(0, len(difficult_words_list), 100):\n",
    "    # print progress based on i, len(difficult_words_list) and 100 per request\n",
    "    print(f\"Processing batch {i // 100 + 1} of {len(difficult_words_list) // 100}\")\n",
    "    batch = difficult_words_list[i:i + 100]\n",
    "    words_str = \", \".join(batch)\n",
    "    prompt = (\n",
    "        f\"Provide a defining translation in English for each of the following words: {words_str}. \"\n",
    "        \"Use no more than three words per defining translation. \"\n",
    "        \"Return the result as a JSON array where each element is an object with the keys 'word' and 'help'. \"\n",
    "        \"Do not include any extra text outside the JSON.\"\n",
    "    )    \n",
    "\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"gemma2-9b-it\",\n",
    "            stream=False,\n",
    "        )\n",
    "        response_text = chat_completion.choices[0].message.content.strip()\n",
    "        definitions = json.loads(response_text)\n",
    "        for entry in definitions:\n",
    "            # Normalize keys to lowercase for consistency.\n",
    "            word_key = entry['word'].lower()\n",
    "            difficult_word_to_help[word_key] = entry['help']\n",
    "    except:\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                stream=False,\n",
    "            )\n",
    "            response_text = chat_completion.choices[0].message.content.strip()\n",
    "            definitions = json.loads(response_text)\n",
    "            for entry in definitions:\n",
    "                # Normalize keys to lowercase for consistency.\n",
    "                word_key = entry['word'].lower()\n",
    "                difficult_word_to_help[word_key] = entry['help']\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to parse JSON for batch: {batch}\")\n",
    "            print(\"Response was:\")\n",
    "            print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Process each XHTML file and replace difficult words with \"original_word [help]\".\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.xhtml'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        # Build a regex pattern matching any difficult word (case-insensitive).\n",
    "        pattern = re.compile(\n",
    "            r'\\b(' + '|'.join(re.escape(word) for word in difficult_word_to_help.keys()) + r')\\b',\n",
    "            flags=re.IGNORECASE\n",
    "        )\n",
    "\n",
    "        def replacer(match):\n",
    "            original_word = match.group(0)\n",
    "            help_text = difficult_word_to_help.get(original_word.lower(), '')\n",
    "            return f\"{original_word} [{help_text}]\" if help_text else original_word\n",
    "\n",
    "        new_content = pattern.sub(replacer, content)\n",
    "\n",
    "        # Overwrite the file with the updated content.\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(new_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
